{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Handwritten digit images form MNIST Dataset using VAE\n",
        "The variational Auto-Encoder is a type of neural network\n",
        "that consists of two neural networks - encoder and a decoder\n",
        "where the encoder applies some transformations to map the\n",
        "input data into lower dimensions and the decoder tries to\n",
        "recreate the input from this data where both encoder and\n",
        "decoder have their own set of learning parameters.\n",
        "• Encoder - The encoder has encoding parameters θ, which\n",
        "maps the high dimensional input data into a low dimensional\n",
        "latent variable representation z through the\n",
        "probability distribution n pθ(z/x). It has its own weights\n",
        "and bias.\n",
        "• Decoder - The decoder has decoding parameters ϕ ,\n",
        "which map the low dimensional latent variable back to\n",
        "the high-dimensional feature space. It generates the new\n",
        "reconstructed data xˆ by p ϕ(x/z) and z.\n"
      ],
      "metadata": {
        "id": "maIGTRyjzqvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the necessary libraries and load the data\n",
        "\n",
        "We import the necessary libraries:\n",
        "* **numpy** - the numerical computing library to provide mathematical operations on arrays \\\n",
        "* **keras** - high level neural network API for training the deep learning models \\\n",
        "* **tensorflow** - open-source library for mchine learning and deep learning \\\n",
        "\n",
        "\n",
        "**MNIST** is a well known dataset for handwritten digitd which we use to train our model to generate new handwritten digit images."
      ],
      "metadata": {
        "id": "Z8QpH1y3zz7s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaoH4FdP8Nsi"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Lambda, Input, Dense, Conv2D, Flatten, Conv2DTranspose, Reshape\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNVH0XohKw60",
        "outputId": "e66e6559-9754-4595-9c0f-53486bb67a73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(x_train_data, y_train_data), (x_test_data, y_test_data) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nVEcUtwO2Tz",
        "outputId": "11c39d89-4ce7-49dd-cdab-6d017069d49b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(784,)\n"
          ]
        }
      ],
      "source": [
        "image_size = x_train_data.shape[1]\n",
        "original_dim = image_size * image_size\n",
        "\n",
        "x_train_data = np.reshape(x_train_data, [-1, original_dim])\n",
        "x_test_data = np.reshape(x_test_data, [-1, original_dim])\n",
        "\n",
        "x_train_data = x_train_data.astype('float32') / 255\n",
        "x_test_data = x_test_data.astype('float32') / 255\n",
        "\n",
        "input_shape = (original_dim, )\n",
        "batch_size = 128\n",
        "print(input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8ijlf-qNQNK"
      },
      "outputs": [],
      "source": [
        "def latent_sampling(args):\n",
        "    z_mean_layer, z_log_var_layer = args\n",
        "    batch = K.shape(z_mean_layer)[0]\n",
        "    dimension = K.int_shape(z_mean_layer)[1]\n",
        "    ep = K.random_normal(shape=(batch, dimension))\n",
        "    return z_mean_layer + K.exp(0.5 * z_log_var_layer) * ep"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Network:\n",
        "In the Variational Autoencoder\n",
        "(VAE), the encoder network is responsible for mapping\n",
        "high-dimensional input data into a lower-dimensional latent\n",
        "space. This is achieved by using hidden layers that applies\n",
        "a nonlinear transformation to the input data to extract its\n",
        "underlying features. The output of the encoder is a set of\n",
        "parameters that define a Gaussian probability distribution in\n",
        "the latent space. These parameters represent the mean and\n",
        "variance of the distribution, and they are used to sample the\n",
        "latent space in a stochastic manner.\n",
        "To improve the performance of the encoder, we\n",
        "experimented with two different architectures: one that\n",
        "uses only ReLU hidden layers and another that uses a\n",
        "combination of Conv2D and ReLU layers. The Conv2D\n",
        "layers allow the encoder to capture spatial features in the\n",
        "input data, while the ReLU layers help to extract higherlevel\n",
        "features. By comparing the performance of these two\n",
        "architectures, we can determine which one is better suited for\n",
        "encoding MNIST images."
      ],
      "metadata": {
        "id": "BA_wZrHuz8gV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYOvXFWyOn7d",
        "outputId": "b3f57fb9-d64e-45d1-d958-b7d7e40ed2a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_encoder (InputLayer)     [(None, 784)]        0           []                               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256)          200960      ['input_encoder[0][0]']          \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 16)           4112        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 16)           4112        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " z (Lambda)                     (None, 16)           0           ['z_mean[0][0]',                 \n",
            "                                                                  'z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 209,184\n",
            "Trainable params: 209,184\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.ops.gen_array_ops import shape\n",
        "hidden_node = 256\n",
        "latent_node = 16\n",
        "# img_shape = (28, 28, 1)\n",
        "\n",
        "# build encoder model\n",
        "# input layer\n",
        "encoder_inputs = Input(shape=input_shape, name='input_encoder')\n",
        "\n",
        "# # encoder architecture 1 - dense hidden layers\n",
        "encoder_hidden = Dense(hidden_node, activation='relu')(encoder_inputs)\n",
        "z_mean = Dense(latent_node, name='z_mean')(encoder_hidden)\n",
        "z_log_var = Dense(latent_node, name='z_log_var')(encoder_hidden)\n",
        "\n",
        "# # encoder architecture 2 -  adding conv2D layers\n",
        "# x = Reshape((28, 28, 1))(encoder_inputs)\n",
        "# x = Conv2D(32, 3, padding='same', activation='relu')(x)\n",
        "# x = Conv2D(64, 3, padding='same', activation='relu', strides=(2, 2))(x)\n",
        "# # x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "# # x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "# shape_before_flattening = K.int_shape(x)\n",
        "# x = Flatten()(x)\n",
        "# x = Dense(16, activation=\"relu\")(x)\n",
        "# z_mean = Dense(latent_node, name=\"z_mean\")(x)\n",
        "# z_log_var = Dense(latent_node, name=\"z_log_var\")(x)\n",
        "#----------------------------------------------\n",
        "\n",
        "\n",
        "# normalize log variance to std dev\n",
        "z = Lambda(latent_sampling, output_shape=(latent_node,), name='z')([z_mean, z_log_var])\n",
        "# instantiate encoder model\n",
        "encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Network\n",
        "\n",
        "The decoder takes the encoded latent\n",
        "representation of a digit as input and reLu activation and\n",
        "sigmoid activation layers are added to the neural net for the\n",
        "model to be able to decode the samples.They produce the\n",
        "784 Bernoulli parameters, one for each of the 784 pixels\n",
        "in the image. However, as the decoder has access only to\n",
        "a summary of the information and not the entire data, it is\n",
        "not able to perfectly reconstruct the input of the encoder.\n",
        "The hidden layers allow the decoder to learn the underlying\n",
        "distribution of the input data and generate a new sample from\n",
        "that distribution, which is then used as the output. This process\n",
        "is based on the principle of maximum likelihood estimation,\n",
        "where the goal is to find the parameters of a distribution that\n",
        "maximize the likelihood of generating the observed data. By\n",
        "using the dense ReLU and sigmoid layers, we are able to\n",
        "achieve a relatively high degree of accuracy in reconstructing\n",
        "the input data, despite the loss of some information due to the\n",
        "summarization of the latent representation."
      ],
      "metadata": {
        "id": "P93scy6f0LXg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7_eAWU9PcUr",
        "outputId": "9ac43a7e-3f5a-4033-da67-d42cee3ca91f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " latent_sampling (InputLayer  [(None, 16)]             0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1568)              26656     \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 7, 7, 32)          0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 14, 14, 64)       18496     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 28, 28, 32)       18464     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 1)        289       \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 784)               615440    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 679,345\n",
            "Trainable params: 679,345\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# build decoder model\n",
        "latent_decoder_inputs = Input(shape=(latent_node,), name='latent_sampling')\n",
        "\n",
        "# decoder architecture 1 - dense hidden layers\n",
        "# decoder_hidden = Dense(hidden_node, activation='relu')(latent_decoder_inputs)\n",
        "# decoder_outputs = Dense(original_dim, activation='sigmoid')(decoder_hidden)\n",
        "\n",
        "# encoder architecture 2 - adding conv2DTranspose layers\n",
        "decoder_hidden = Dense(units=7*7*32, activation='relu')(latent_decoder_inputs)\n",
        "decoder_hidden = Reshape(target_shape=(7, 7, 32))(decoder_hidden)\n",
        "decoder_hidden = Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same',activation='relu')(decoder_hidden)\n",
        "decoder_hidden = Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same',activation='relu')(decoder_hidden)\n",
        "decoder_hidden = Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same')(decoder_hidden)\n",
        "flatten_layer = Flatten()(decoder_hidden)\n",
        "decoder_outputs = Dense(original_dim, activation='sigmoid')(flatten_layer)\n",
        "#-------------\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = Model(latent_decoder_inputs, decoder_outputs, name='decoder')\n",
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### instantiate VAE model\n",
        "\n",
        "The VAE is trained using maximum likelihood\n",
        "estimation (MLE) with an additional KL divergence\n",
        "regularization term. The training process involves minimizing\n",
        "the negative log-likelihood of the data under the model,\n",
        "which is equivalent to maximizing the evidence lower bound\n",
        "(ELBO) objective. The ELBO is defined as the sum of the\n",
        "reconstruction loss and the KL divergence between the learned\n",
        "latent distribution and the prior distribution. The crucial point\n",
        "of VAEs is to learn an encoder that can aggregate posterior\n",
        "distribution q(z) = ED[q(z/x)] close to the prior p(z). This\n",
        "is monitored by the loss function of the VAE. In VAE,\n",
        "the reconstruction error between the input data x and the\n",
        "encoded-decoded data d(e(x)) is represented as (d(e(x))). The\n",
        "dimensionality reduction problem here can be written as:\n",
        "(e∗, d∗) = argmin(e, d)inE ∗ D(d(e(x))) (1)\n",
        "This reconstruction error is back propagated to the network,\n",
        "after it is reparameterized."
      ],
      "metadata": {
        "id": "U0KzC4jU0ZdM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nypSF5zrTkPj",
        "outputId": "482d795e-b409-4ea4-b701-eaccbf6c4610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"vae_mlp\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_encoder (InputLayer)     [(None, 784)]        0           []                               \n",
            "                                                                                                  \n",
            " encoder (Functional)           [(None, 16),         209184      ['input_encoder[0][0]']          \n",
            "                                 (None, 16),                                                      \n",
            "                                 (None, 16)]                                                      \n",
            "                                                                                                  \n",
            " decoder (Functional)           (None, 784)          679345      ['encoder[0][2]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256)          200960      ['input_encoder[0][0]']          \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 16)           4112        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 16)           4112        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 16)          0           ['z_log_var[0][0]']              \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.math.square (TFOpLambda)    (None, 16)           0           ['z_mean[0][0]']                 \n",
            "                                                                                                  \n",
            " tf.cast (TFOpLambda)           (None, 784)          0           ['input_encoder[0][0]']          \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor (TFOpLamb  (None, 784)         0           ['decoder[0][0]']                \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.math.subtract (TFOpLambda)  (None, 16)           0           ['tf.__operators__.add[0][0]',   \n",
            "                                                                  'tf.math.square[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.exp (TFOpLambda)       (None, 16)           0           ['z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            " tf.keras.backend.binary_crosse  (None, 784)         0           ['tf.cast[0][0]',                \n",
            " ntropy (TFOpLambda)                                              'tf.convert_to_tensor[0][0]']   \n",
            "                                                                                                  \n",
            " tf.math.subtract_1 (TFOpLambda  (None, 16)          0           ['tf.math.subtract[0][0]',       \n",
            " )                                                                'tf.math.exp[0][0]']            \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpLambd  (None,)             0           ['tf.keras.backend.binary_crossen\n",
            " a)                                                              tropy[0][0]']                    \n",
            "                                                                                                  \n",
            " tf.math.reduce_sum (TFOpLambda  (None,)             0           ['tf.math.subtract_1[0][0]']     \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLambda)  (None,)              0           ['tf.math.reduce_mean[0][0]']    \n",
            "                                                                                                  \n",
            " tf.math.multiply_1 (TFOpLambda  (None,)             0           ['tf.math.reduce_sum[0][0]']     \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None,)             0           ['tf.math.multiply[0][0]',       \n",
            " mbda)                                                            'tf.math.multiply_1[0][0]']     \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_1 (TFOpLam  ()                  0           ['tf.__operators__.add_1[0][0]'] \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " add_loss (AddLoss)             ()                   0           ['tf.math.reduce_mean_1[0][0]']  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 888,529\n",
            "Trainable params: 888,529\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# instantiate VAE model\n",
        "\n",
        "model_outputs = decoder(encoder(encoder_inputs)[2])\n",
        "vae = Model(encoder_inputs, model_outputs, name='vae_mlp')\n",
        "\n",
        "reconstruction_loss = binary_crossentropy(encoder_inputs, model_outputs)\n",
        "\n",
        "reconstruction_loss *= original_dim\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')\n",
        "vae.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdsZY5pCRyT-"
      },
      "outputs": [],
      "source": [
        "def train_model(model):\n",
        "    trained_model = model.fit(x_train_data, epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(x_test_data, None))\n",
        "    return trained_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S_hEQB-RzqD"
      },
      "outputs": [],
      "source": [
        "def plot_latent_space(encoder):\n",
        "    z_test, _, _ = encoder.predict(x_test_data, batch_size=batch_size)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_test[:, 0], z_test[:, 1], c=y_test_data)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"Z[0]\")\n",
        "    plt.ylabel(\"Z[1]\")\n",
        "    plt.savefig(\"VAE\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcM0UkoIR5qJ"
      },
      "outputs": [],
      "source": [
        "def plot_vae_output(decoder,latent_dim):\n",
        "    n = 1\n",
        "    digit_num = 28\n",
        "    figure = np.zeros((digit_num * n, digit_num * n))\n",
        "    axis_x = np.linspace(-3, 3, n)\n",
        "    axis_y = np.linspace(-3, 3, n)[::-1]\n",
        "    for i, yi in enumerate(axis_y):\n",
        "        for j, xi in enumerate(axis_x):\n",
        "            if(latent_dim == 2):\n",
        "                latent_sample = np.array([[xi, yi]*int(latent_dim/2)])\n",
        "            else:\n",
        "                latent_sample = np.random.normal(0,1,size=[batch_size, latent_dim])\n",
        "            input_decoded = decoder.predict(latent_sample)\n",
        "\n",
        "            digit = input_decoded[0].reshape(digit_num, digit_num)\n",
        "            figure[i * digit_num: (i + 1) * digit_num,\n",
        "                   j * digit_num: (j + 1) * digit_num] = digit\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    i_range = digit_num // 2\n",
        "    j_range = (n - 1) * digit_num + i_range + 1\n",
        "    pixel_range = np.arange(i_range, j_range, digit_num)\n",
        "    sample_range_x = np.round(axis_x, 1)\n",
        "    sample_range_y = np.round(axis_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.imshow(figure, cmap='gray')\n",
        "    plt.savefig(\"VAE_Output\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model and plot the results."
      ],
      "metadata": {
        "id": "3lZh3jml4kGw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTw_pgvCSNNR",
        "outputId": "cdef2428-ad6f-4323-dcc9-18080d82b02d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "469/469 [==============================] - 115s 240ms/step - loss: 179.9852 - val_loss: 134.0219\n",
            "Epoch 2/500\n",
            "469/469 [==============================] - 111s 237ms/step - loss: 122.7176 - val_loss: 115.1915\n",
            "Epoch 3/500\n",
            "469/469 [==============================] - 109s 233ms/step - loss: 113.5444 - val_loss: 110.4667\n",
            "Epoch 4/500\n",
            "469/469 [==============================] - 117s 250ms/step - loss: 110.1329 - val_loss: 108.3895\n",
            "Epoch 5/500\n",
            "469/469 [==============================] - 109s 233ms/step - loss: 108.3438 - val_loss: 106.5858\n",
            "Epoch 6/500\n",
            "469/469 [==============================] - 110s 234ms/step - loss: 107.0303 - val_loss: 105.7444\n",
            "Epoch 7/500\n",
            "469/469 [==============================] - 111s 237ms/step - loss: 106.0607 - val_loss: 105.1544\n",
            "Epoch 8/500\n",
            "469/469 [==============================] - 110s 234ms/step - loss: 105.2999 - val_loss: 104.7648\n",
            "Epoch 9/500\n",
            "469/469 [==============================] - 106s 226ms/step - loss: 104.6146 - val_loss: 104.0237\n",
            "Epoch 10/500\n",
            "469/469 [==============================] - 105s 223ms/step - loss: 104.0764 - val_loss: 103.5843\n",
            "Epoch 11/500\n",
            "469/469 [==============================] - 104s 221ms/step - loss: 103.5511 - val_loss: 103.0696\n",
            "Epoch 12/500\n",
            "469/469 [==============================] - 98s 209ms/step - loss: 103.0166 - val_loss: 102.6032\n",
            "Epoch 13/500\n",
            "469/469 [==============================] - 99s 211ms/step - loss: 102.5919 - val_loss: 102.2897\n",
            "Epoch 14/500\n",
            "469/469 [==============================] - 97s 207ms/step - loss: 102.2131 - val_loss: 101.8908\n",
            "Epoch 15/500\n",
            "469/469 [==============================] - 103s 220ms/step - loss: 101.8508 - val_loss: 101.7790\n",
            "Epoch 16/500\n",
            "469/469 [==============================] - 104s 221ms/step - loss: 101.5228 - val_loss: 101.4280\n",
            "Epoch 17/500\n",
            "469/469 [==============================] - 99s 211ms/step - loss: 101.2616 - val_loss: 101.1865\n",
            "Epoch 18/500\n",
            "469/469 [==============================] - 100s 213ms/step - loss: 100.9755 - val_loss: 100.9889\n",
            "Epoch 19/500\n",
            "469/469 [==============================] - 100s 214ms/step - loss: 100.7174 - val_loss: 100.7847\n",
            "Epoch 20/500\n",
            "469/469 [==============================] - 97s 208ms/step - loss: 100.4458 - val_loss: 100.7108\n",
            "Epoch 21/500\n",
            "469/469 [==============================] - 101s 215ms/step - loss: 100.2393 - val_loss: 100.7604\n",
            "Epoch 22/500\n",
            "469/469 [==============================] - 99s 210ms/step - loss: 100.0581 - val_loss: 100.4150\n",
            "Epoch 23/500\n",
            "469/469 [==============================] - 101s 215ms/step - loss: 99.9424 - val_loss: 100.3381\n",
            "Epoch 24/500\n",
            "469/469 [==============================] - 100s 213ms/step - loss: 99.7305 - val_loss: 100.0782\n",
            "Epoch 25/500\n",
            "469/469 [==============================] - 101s 216ms/step - loss: 99.5960 - val_loss: 100.0389\n",
            "Epoch 26/500\n",
            "469/469 [==============================] - 94s 202ms/step - loss: 99.4308 - val_loss: 99.8924\n",
            "Epoch 27/500\n",
            "469/469 [==============================] - 95s 203ms/step - loss: 99.3416 - val_loss: 99.9604\n",
            "Epoch 28/500\n",
            "469/469 [==============================] - 95s 203ms/step - loss: 99.1702 - val_loss: 99.8561\n",
            "Epoch 29/500\n",
            "469/469 [==============================] - 95s 203ms/step - loss: 99.0708 - val_loss: 99.7794\n",
            "Epoch 30/500\n",
            "469/469 [==============================] - 95s 203ms/step - loss: 98.9192 - val_loss: 99.7037\n",
            "Epoch 31/500\n",
            "469/469 [==============================] - 95s 203ms/step - loss: 98.8468 - val_loss: 99.6784\n",
            "Epoch 32/500\n",
            "469/469 [==============================] - 95s 202ms/step - loss: 98.7496 - val_loss: 99.4938\n",
            "Epoch 33/500\n",
            "469/469 [==============================] - 99s 212ms/step - loss: 98.6253 - val_loss: 99.3734\n",
            "Epoch 34/500\n",
            "469/469 [==============================] - 97s 207ms/step - loss: 98.5561 - val_loss: 99.3429\n",
            "Epoch 35/500\n",
            "469/469 [==============================] - 99s 212ms/step - loss: 98.4827 - val_loss: 99.3867\n",
            "Epoch 36/500\n",
            "469/469 [==============================] - 100s 214ms/step - loss: 98.3824 - val_loss: 99.3486\n",
            "Epoch 37/500\n",
            "469/469 [==============================] - 96s 204ms/step - loss: 98.2844 - val_loss: 99.2427\n",
            "Epoch 38/500\n",
            "469/469 [==============================] - 95s 203ms/step - loss: 98.2280 - val_loss: 99.1260\n",
            "Epoch 39/500\n",
            "469/469 [==============================] - 96s 204ms/step - loss: 98.1313 - val_loss: 99.1083\n",
            "Epoch 40/500\n",
            "469/469 [==============================] - 96s 204ms/step - loss: 98.1039 - val_loss: 99.0494\n",
            "Epoch 41/500\n",
            "469/469 [==============================] - 101s 215ms/step - loss: 98.0517 - val_loss: 99.0538\n",
            "Epoch 42/500\n",
            "469/469 [==============================] - 100s 214ms/step - loss: 97.9574 - val_loss: 99.1626\n",
            "Epoch 43/500\n",
            "469/469 [==============================] - 96s 204ms/step - loss: 97.8665 - val_loss: 99.0966\n",
            "Epoch 44/500\n",
            "469/469 [==============================] - 96s 205ms/step - loss: 97.7988 - val_loss: 99.2548\n",
            "Epoch 45/500\n",
            "469/469 [==============================] - 98s 209ms/step - loss: 97.7975 - val_loss: 98.9093\n",
            "Epoch 46/500\n",
            "469/469 [==============================] - 99s 212ms/step - loss: 97.7629 - val_loss: 98.8113\n",
            "Epoch 47/500\n",
            "469/469 [==============================] - 96s 204ms/step - loss: 97.6471 - val_loss: 98.8733\n",
            "Epoch 48/500\n",
            "469/469 [==============================] - 97s 206ms/step - loss: 97.5942 - val_loss: 98.7781\n",
            "Epoch 49/500\n",
            "469/469 [==============================] - 102s 217ms/step - loss: 97.5659 - val_loss: 98.8297\n",
            "Epoch 50/500\n",
            "469/469 [==============================] - 100s 214ms/step - loss: 97.5349 - val_loss: 98.8202\n",
            "Epoch 51/500\n",
            "469/469 [==============================] - 96s 206ms/step - loss: 97.5192 - val_loss: 98.7916\n",
            "Epoch 52/500\n",
            "469/469 [==============================] - 101s 215ms/step - loss: 97.4356 - val_loss: 98.8622\n",
            "Epoch 53/500\n",
            "469/469 [==============================] - 107s 229ms/step - loss: 97.4060 - val_loss: 99.0180\n",
            "Epoch 54/500\n",
            "469/469 [==============================] - 98s 208ms/step - loss: 97.3569 - val_loss: 98.8242\n",
            "Epoch 55/500\n",
            "469/469 [==============================] - 96s 205ms/step - loss: 97.3185 - val_loss: 98.7970\n",
            "Epoch 56/500\n",
            "469/469 [==============================] - 100s 214ms/step - loss: 97.3205 - val_loss: 98.7361\n",
            "Epoch 57/500\n",
            "469/469 [==============================] - 97s 206ms/step - loss: 97.2329 - val_loss: 98.7509\n",
            "Epoch 58/500\n",
            "469/469 [==============================] - 97s 206ms/step - loss: 97.2175 - val_loss: 98.7729\n",
            "Epoch 59/500\n",
            "469/469 [==============================] - 96s 205ms/step - loss: 97.1738 - val_loss: 98.9058\n",
            "Epoch 60/500\n",
            "469/469 [==============================] - 96s 204ms/step - loss: 97.1251 - val_loss: 98.9779\n",
            "Epoch 61/500\n",
            "469/469 [==============================] - 94s 201ms/step - loss: 97.1452 - val_loss: 98.5515\n",
            "Epoch 62/500\n",
            "469/469 [==============================] - 97s 207ms/step - loss: 97.0607 - val_loss: 98.6684\n",
            "Epoch 63/500\n",
            "469/469 [==============================] - 95s 203ms/step - loss: 97.0676 - val_loss: 98.3787\n",
            "Epoch 64/500\n",
            "469/469 [==============================] - 94s 201ms/step - loss: 96.9936 - val_loss: 98.6198\n",
            "Epoch 65/500\n",
            "469/469 [==============================] - 95s 202ms/step - loss: 96.9409 - val_loss: 98.4484\n",
            "Epoch 66/500\n",
            "469/469 [==============================] - 93s 198ms/step - loss: 96.9258 - val_loss: 98.5042\n",
            "Epoch 67/500\n",
            "469/469 [==============================] - 96s 206ms/step - loss: 96.8686 - val_loss: 98.5222\n",
            "Epoch 68/500\n",
            "469/469 [==============================] - 99s 212ms/step - loss: 96.8530 - val_loss: 98.5823\n",
            "Epoch 69/500\n",
            "469/469 [==============================] - 100s 214ms/step - loss: 96.8300 - val_loss: 98.4014\n",
            "Epoch 70/500\n",
            "469/469 [==============================] - 103s 219ms/step - loss: 96.8434 - val_loss: 98.4378\n",
            "Epoch 71/500\n",
            "469/469 [==============================] - 101s 216ms/step - loss: 96.7858 - val_loss: 98.5671\n",
            "Epoch 72/500\n",
            "469/469 [==============================] - 100s 213ms/step - loss: 96.7633 - val_loss: 98.3338\n",
            "Epoch 73/500\n",
            " 84/469 [====>.........................] - ETA: 1:19 - loss: 96.3628"
          ]
        }
      ],
      "source": [
        "epochs = 500\n",
        "trained_model = train_model(vae)\n",
        "plot_latent_space(encoder)\n",
        "plot_vae_output(decoder, latent_node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9xUkCP8_XYcP",
        "outputId": "ac74982a-1f4b-4cfd-8256-6eccee3c7e6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(trained_model.history['loss'], label='Training Loss')\n",
        "plt.plot(trained_model.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}